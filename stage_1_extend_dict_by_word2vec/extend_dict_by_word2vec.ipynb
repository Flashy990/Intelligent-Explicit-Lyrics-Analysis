{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DuXJP1lfzvS7",
    "outputId": "0f11bc75-7dab-42da-bfb7-52cbd3b4e500"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Seed Words with Original Dictionary: {'Sexual': ['one night stand', 'pound', 'cumshot', 'ejaculate', 'wearin', 'occasions', 'attraction', 'underwear', 'sexual act', 'blow job', 'pistol', 'devoted', 'ass', 'dick', 'balls', 'dink', 'hopelessly', 'cabbage', 'thugged', 'erotic', 'sensual', 'masturbate', 'panties', 'handjob', 'breast', 'masturbation', 'provocative', 'sex', 'vagina', 'nudity', 'twins', 'nude', 'virgin', 'flirt', 'seductive', 'brassiere', 'dabei', 'make love', 'dub', 'bra', 'orgasm', 'socks', 'rappers', 'mask'], 'Violence': ['gore', 'combat', 'atlanta', 'violence', 'atrocity', 'punch', 'teachers', 'gun', 'knife', 'murder', 'assault', 'ð½ñ', 'stab', 'rat', 'luftballons', 'violent', 'gin', 'attack', 'chillin', 'pasito', 'mutilation', 'fikir', 'battle', 'korea', 'devilry', 'weapons', 'kill'], 'Substance': ['card', 'heroin', 'intoxicated', 'beer', 'guajira', 'foreign', 'del', 'vodka', 'alcohol', 'vamos', 'disease', 'ritz', 'balls', 'trickin', 'dealers', 'opioids', 'ecstasy', 'cigar', 'marijuana', 'smoke', 'nos', 'booze', 'weed', 'å¹', 'drugs', 'poquito', 'drunk', 'meth', 'cocaine', 'bentleys', 'oooooooh', 'metal', 'clip', 'pills', 'guantanamera'], 'Language': ['asshole', 'violence', 'slander', 'defamatory', 'teachers', 'gucci', 'racial', 'cristal', 'slur', 'nigga', 'vulgar', 'ð½ñ', 'motherfucker', 'luftballons', 'mocking', 'abusive', 'bastard', 'expletive', 'fuck', 'shit', 'pasito', 'virgin', 'parkin', 'foul', 'obscene', 'korea', 'contemptible', 'rappin', 'houston', 'wretch', 'damn']}\n",
      "Expanded seed words saved to expanded_seed_words.csv\n"
     ]
    }
   ],
   "source": [
    "# Required installations and imports\n",
    "!pip install gensim\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the word dictionary CSV file\n",
    "word_dict_path = 'word_dict.csv'\n",
    "word_dict_df = pd.read_csv(word_dict_path, encoding='utf-8')\n",
    "\n",
    "# Convert the CSV data into a dictionary format\n",
    "seed_words = {col: word_dict_df[col].dropna().tolist() for col in word_dict_df.columns}\n",
    "\n",
    "# Download stopwords and set up stopword list\n",
    "all_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess text function for Word2Vec training\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W+', ' ', str(text).lower())\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 1 and not token.isnumeric()]\n",
    "    tokens = [token for token in tokens if token not in all_stopwords]\n",
    "    return tokens\n",
    "\n",
    "# Load the lyrics dataset from your uploaded file\n",
    "lyrics_path = 'clean_unlabeled_utf8.csv'\n",
    "df = pd.read_csv(lyrics_path, encoding='utf-8')\n",
    "df['processed_lyrics'] = df['lyrics'].apply(preprocess_text)\n",
    "\n",
    "# Train a Word2Vec model using the processed lyrics\n",
    "all_lyrics = df['processed_lyrics'].tolist()\n",
    "w2v_model = Word2Vec(sentences=all_lyrics, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to expand seed words with top5 and high similarity threshold\n",
    "def expand_seed_words(seed_words, model, top_n=5, similarity_threshold=0.7):\n",
    "    expanded_words = {}\n",
    "    for category, words in seed_words.items():\n",
    "        expanded_set = set(words) \n",
    "        for word in words:\n",
    "            if word in model.wv:\n",
    "                similar_words = model.wv.most_similar(word, topn=top_n)\n",
    "                expanded_set.update([w for w, sim in similar_words if sim >= similarity_threshold])\n",
    "        expanded_words[category] = list(expanded_set)\n",
    "    return expanded_words\n",
    "\n",
    "# Expand seed words using the Word2Vec model\n",
    "expanded_seed_words = expand_seed_words(seed_words, w2v_model, top_n=5, similarity_threshold=0.95)\n",
    "\n",
    "# Display the final expanded dictionary\n",
    "print(\"Expanded Seed Words with Original Dictionary:\", expanded_seed_words)\n",
    "\n",
    "# Save expanded seed words as a new CSV file (optional)\n",
    "expanded_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in expanded_seed_words.items()]))\n",
    "expanded_df.to_csv('expanded_seed_words.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"Expanded seed words saved to expanded_seed_words.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
